{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f922bba",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dad0912a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f5f52a54790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import lucene\n",
    "import time\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from java.io import File\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.util import BytesRefIterator\n",
    "from org.apache.lucene.index import DirectoryReader, Term\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.analysis.core import WhitespaceAnalyzer\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.search import IndexSearcher, BooleanQuery, BooleanClause, TermQuery, BoostQuery\n",
    "from org.apache.lucene.search.similarities import BM25Similarity, LMJelinekMercerSimilarity, LMDirichletSimilarity\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daecea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_name = 'trec6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da8c844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = '../../../index/'\n",
    "topicFilePath = f'../../../{q_name}.xml'\n",
    "qrel_file = '../../../trec678_robust.qrel'\n",
    "\n",
    "directory = FSDirectory.open(File(index_path).toPath())\n",
    "indexReader = DirectoryReader.open(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3061de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_topics(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    parsed_topics = {}\n",
    "\n",
    "    for top in root.findall('top'):\n",
    "        num = top.find('num').text.strip()\n",
    "        title = top.find('title').text.strip()\n",
    "        parsed_topics[num] = title\n",
    "\n",
    "    return parsed_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9be077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRelJudgeDict(qrelFilePath):\n",
    "    # {qid1:{docid1:0/1,docid2:0/1,...}, qid2:{docid2:0/1,docid4:0/1,...},...}\n",
    "    relJudgeDict = {}\n",
    "    with open(qrelFilePath, 'r') as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            qid, docid, judgement = l[0], l[2], int(l[3])\n",
    "            if qid not in relJudgeDict:\n",
    "                relJudgeDict[qid] = {docid: judgement}\n",
    "            else:\n",
    "                relJudgeDict[qid][docid] = judgement\n",
    "    return relJudgeDict\n",
    "\n",
    "def isTrueRelevant(qid, docid, relJudgeDict):\n",
    "    # returns if the doc is True relevant, for the given query, according to the judgment file\n",
    "    if qid not in relJudgeDict:\n",
    "        return False\n",
    "    if docid not in relJudgeDict[qid]:\n",
    "        return False\n",
    "    if relJudgeDict[qid][docid] == 1:   # 1 -> Relevant TRF\n",
    "        return True\n",
    "    if relJudgeDict[qid][docid] == 0:\n",
    "        return False\n",
    "\n",
    "def isTrueNonRelevant(qid, docid, relJudgeDict):\n",
    "    # returns if the doc is NOT true relevant, for the given query, according to the judgment file\n",
    "    if qid not in relJudgeDict:\n",
    "        return False\n",
    "    if docid not in relJudgeDict[qid]:\n",
    "        return False\n",
    "    if relJudgeDict[qid][docid] == 0:   # 0 -> Non-relevant TRF\n",
    "        return True\n",
    "    if relJudgeDict[qid][docid] == 1:\n",
    "        return False\n",
    "\n",
    "relJudgeDict = makeRelJudgeDict(qrel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64d41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all = query_topics(topicFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64bc6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentVector(luceneDocid, indexReader):               \n",
    "    \n",
    "    docVec = {}\n",
    "    D = 0                                 \n",
    "    \n",
    "    terms = indexReader.getTermVector(luceneDocid, 'CONTENTS')\n",
    "    iterator = terms.iterator()\n",
    "    for term in BytesRefIterator.cast_(iterator):\n",
    "        t = term.utf8ToString()\n",
    "        tf = iterator.totalTermFreq()  \n",
    "        D += tf\n",
    "        docVec[t] = tf\n",
    "    \n",
    "    docVec = {key: (value / D)  for key, value in docVec.items()}\n",
    "\n",
    "    return docVec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "265bc659",
   "metadata": {},
   "source": [
    "### RM3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "076c7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(indexReader, query, similarity, top_rel_doc, qid, tpd, tnd):\n",
    "    analyzer = EnglishAnalyzer()\n",
    "    searcher = IndexSearcher(indexReader)\n",
    "    searcher.setSimilarity(similarity)\n",
    "    # query = QueryParser('CONTENTS', analyzer).escape(query)      # a few titles had '/' in them which \n",
    "    \n",
    "    # query = QueryParser(\"CONTENTS\", analyzer).parse(query)\n",
    "\n",
    "    scoreDocs = searcher.search(query, top_rel_doc).scoreDocs\n",
    "    \n",
    "    docids = [scoreDoc.doc for scoreDoc in scoreDocs]\n",
    "\n",
    "    relevent_docs = []\n",
    "    nonrel_docs = []\n",
    "    for id in docids:\n",
    "        doc = searcher.doc(id)\n",
    "        if isTrueRelevant(qid, doc.get('ID'), relJudgeDict):\n",
    "            relevent_docs.append(id)\n",
    "        if isTrueNonRelevant(qid, doc.get('ID'), relJudgeDict):\n",
    "            nonrel_docs.append(id)\n",
    "   \n",
    "    # print(qid,relevent_docs)\n",
    "    docids = relevent_docs[:tpd]\n",
    "    ndocid = nonrel_docs[:tnd]\n",
    "    \n",
    "    set_cont = {term for doc in docids for term in getDocumentVector(doc, indexReader).keys()}\n",
    "\n",
    "    \n",
    "    \n",
    "    # set_n = {term for doc in ndocid for term in getDocumentVector(doc, indexReader).keys()}\n",
    "    \n",
    "    # query_terms_set = set([term.strip()[9:] for term in query.toString().split()])\n",
    "    # set_cont = set_cont.difference(set_n) | query_terms_set\n",
    "\n",
    "    # set_cont = {ele for ele in set_cont if ele.isalpha()} \n",
    "\n",
    "    return set_cont, docids, ndocid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b873b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RM3_term_selection(Query, set_ET, docs, ndocid, indexReader, alpha, lamb, expanded_query_terms):\n",
    "    \n",
    "    totalTF = indexReader.getSumTotalTermFreq(\"CONTENTS\")\n",
    "\n",
    "    Q = Query.split()\n",
    "    weight = {}\n",
    "\n",
    "    cf = {}\n",
    "    for t in set_ET | set(Q):\n",
    "        T = Term(\"CONTENTS\", t)\n",
    "        cf[t] = indexReader.totalTermFreq(T)/totalTF\n",
    "\n",
    "    docVectors = {}\n",
    "    ndocVectors = {}\n",
    "   \n",
    "    \n",
    "    for d in docs:                    \n",
    "        docVectors[d] = getDocumentVector(d, indexReader)\n",
    "    \n",
    "    for d in ndocid:                    \n",
    "        ndocVectors[d] = getDocumentVector(d, indexReader)\n",
    "        \n",
    "        \n",
    "    # tagged = nltk.tag.pos_tag(list(set_ET), tagset='universal')\n",
    "    # set_ET = set([ele[0] for ele in tagged if ele[1] in ['NOUN','ADJ','ADP','X']])\n",
    "    ml = lamb\n",
    "    for w in set_ET:\n",
    "        p_wr = 0\n",
    "        tdc = 0\n",
    "        for d in docs:                  \n",
    "            tdc = tdc + 1\n",
    "            # p_wd = (ml*(docVectors[d].get(w,0)) + (1 - ml)*cf[w]) \n",
    "            p_wd = docVectors[d].get(w,0)     \n",
    "        \n",
    "            p_q = 1\n",
    "            for q in Q:\n",
    "                # p_q = p_q*docVectors[d].get(q,0)   \n",
    "                p_q = p_q*(ml*(docVectors[d].get(q,0)) + (1 - ml)*cf[q])   \n",
    "\n",
    "            p_wr = p_wr + p_wd\n",
    "        \n",
    "        p_wr = p_wr/tdc\n",
    "\n",
    "        p_wnr = 0\n",
    "        ndc = 0 \n",
    "        for d in ndocid:\n",
    "            ndc = ndc + 1\n",
    "            \n",
    "            # p_wnd = (ml*(ndocVectors[d].get(w,0)) + (1 - ml)*cf[w]) \n",
    "            p_wnd = ndocVectors[d].get(w,0)\n",
    "\n",
    "            p_q = 1\n",
    "            for q in Q:\n",
    "                # p_q = p_q*docVectors[d].get(q,0)   \n",
    "                p_q = p_q*(ml*(ndocVectors[d].get(q,0)) + (1 - ml)*cf[q])\n",
    "\n",
    "            p_wnr = p_wnr + p_wnd\n",
    "        if ndc != 0:\n",
    "            p_wnr = p_wnr/ndc\n",
    "        \n",
    "        p_wr = p_wr - p_wnr\n",
    "\n",
    "        if p_wr > 0:\n",
    "            weight[w] = p_wr\n",
    "\n",
    "\n",
    "    weight = dict(sorted(weight.items(), key=lambda x:x[1], reverse=True)[:expanded_query_terms])\n",
    "    \n",
    "    norm = sum(weight.values())\n",
    "    weight = {w:weight[w]/norm for w in weight}\n",
    " \n",
    "    for w in weight.keys() | set(Q):\n",
    "        weight[w] = (alpha*weight.get(w,0)) + (1-alpha)*(Q.count(w)/len(Q))\n",
    "  \n",
    "\n",
    "    temp_list = sorted(weight.items(), key=lambda x:x[1], reverse=True)\n",
    "    sorted_weights = dict(temp_list)\n",
    "\n",
    "    return sorted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37a65a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanded_query_BM25(search, RM3_term_selection, k1, b, alpha, top_rel_doc, expanded_query_terms, mu, tpd, tnd):\n",
    "\n",
    "    analyzer = EnglishAnalyzer()\n",
    "    similarity = BM25Similarity(k1,b)\n",
    "    expanded_q = []\n",
    "\n",
    "    i = 0\n",
    "    for qid, q in tqdm(query_all.items(), colour='red', desc='Expanding Queries', leave=False):\n",
    "    # for qid, q in query_all.items():\n",
    "     \n",
    "        i += 1 \n",
    "        escaped_q = QueryParser('CONTENTS', analyzer).escape(q)      # a few titles had '/' in them which \n",
    "        query = QueryParser('CONTENTS', analyzer).parse(escaped_q)\n",
    "        \n",
    "        query_terms = [term.strip()[9:] for term in query.toString().split()]\n",
    "        parsed_q = ' '.join(query_terms)\n",
    "#         print(parsed_q)\n",
    "        expension_term_set, docids, ndocid = search(indexReader, query, similarity, top_rel_doc, qid, tpd, tnd)\n",
    "        # expension_term_set, docids, ndocid = search(indexReader, q, similarity, top_rel_doc, qid, tpd, tnd)\n",
    "        weights = RM3_term_selection(parsed_q, expension_term_set, docids, ndocid, indexReader, alpha, mu, expanded_query_terms)\n",
    "    \n",
    "        # print(weights.keys())    \n",
    "        booleanQuery = BooleanQuery.Builder()\n",
    "        for m, n in weights.items():\n",
    "            t = Term('CONTENTS', m)\n",
    "            tq = TermQuery(t)\n",
    "            boostedTermQuery = BoostQuery(tq, float(n))\n",
    "            BooleanQuery.setMaxClauseCount(4096)\n",
    "            booleanQuery.add(boostedTermQuery, BooleanClause.Occur.SHOULD)\n",
    "        booleanQuery = booleanQuery.build()\n",
    "       \n",
    "        expanded_q.append(booleanQuery)   \n",
    "\n",
    "    return expanded_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "108049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_retrived(indexReader, Query, Qid, similarity, out_name):\n",
    "\n",
    "    searcher = IndexSearcher(indexReader)\n",
    "    searcher.setSimilarity(similarity)\n",
    "   \n",
    "    scoreDocs = searcher.search(Query, 1000).scoreDocs             #retrieving top 1000 relDoc\n",
    "    i = 1\n",
    "    res = ''\n",
    "\n",
    "    for scoreDoc in scoreDocs:\n",
    "        doc = searcher.doc(scoreDoc.doc)\n",
    "        r = str(Qid) + '\\t' + 'Q0' + '\\t' + str(doc.get('ID')) + '\\t' + str(i) + '\\t' + str(scoreDoc.score) + '\\t' + str(out_name) + '\\n'\n",
    "        res += r\n",
    "        i = i+1   \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a67596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RM3(top_PRD, expanded_query_terms, alpha, lamb, tpd, tnd):\n",
    "    expand_q = expanded_query_BM25(search, RM3_term_selection, k1, b, alpha, top_PRD, expanded_query_terms, lamb, tpd, tnd)\n",
    "                                       \n",
    "    name = 'prm_'\n",
    "    sim = BM25Similarity(k1,b)\n",
    "    name = name + 'BM25_' + str(k1) + '_'+ str(b)\n",
    "\n",
    "    file_name = f'./res_TRF/{q_name}/{tpd}_{tnd}_{q_name}_lamb_' + str(lamb) +'_docs_' + str(top_PRD) + '_terms_' + str(expanded_query_terms) + '_alpha_' + str(alpha) +'_tf' +'.txt'\n",
    "    out_file = open(file_name, \"w\")\n",
    "\n",
    "    res = ''\n",
    "    for i in tqdm(range(len(query_all)),colour='cyan', desc = 'Re-retrival', leave=False):\n",
    "    # for i in range(len(query_all)):\n",
    "    \n",
    "        result =  search_retrived(indexReader, expand_q[i], list(query_all.keys())[i], sim, name)\n",
    "        res = res + result\n",
    "\n",
    "    out_file.write(res)\n",
    "    out_file.close()\n",
    "    # print(\"Retrieval Completed - result dumped in\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a113aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[31m██████████\u001b[0m| 3/3 [01:23<00:00, 27.79s/it]\n"
     ]
    }
   ],
   "source": [
    "k1 = 0.8\n",
    "b = 0.4\n",
    "\n",
    "tpd = [20]\n",
    "tnd = [0, 5 ,20]\n",
    "top_PRD = [1000]\n",
    "expanded_query_terms = [50]\n",
    "alpha = [0.8]\n",
    "lamb = [0.7]\n",
    "\n",
    "parameters = list(itertools.product(top_PRD, expanded_query_terms, alpha, lamb, tpd, tnd))\n",
    "\n",
    "for num_doc, num_q, alpha, lamb, tpd, tnd in tqdm(parameters, colour='red'):\n",
    "    run_RM3(num_doc, num_q, alpha, lamb, tpd, tnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e388db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
